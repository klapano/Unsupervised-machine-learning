{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1ec43e",
   "metadata": {},
   "source": [
    "2) Hierarchical Clustering: Hierarchical clustering is a clustering algorithm that builds a hierarchy of clusters by merging or splitting them successively. It's often used for exploratory data analysis, gene clustering, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "X = np.random.randn(50, 2)\n",
    "\n",
    "# Create an AgglomerativeClustering model and fit it to the data\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "agg_clustering.fit(X)\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "labels = agg_clustering.labels_\n",
    "\n",
    "# Plot the data points and color them according to the cluster assignment\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(3):\n",
    "    plt.scatter(X[labels == i, 0], X[labels == i, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395afa5",
   "metadata": {},
   "source": [
    "In this example, the AgglomerativeClustering class is used to perform hierarchical clustering on a dataset of 2D points. The number of clusters is set to 3, and the model is fit to the data using the fit method. The labels_ attribute of the model is used to get the cluster assignments for each data point. Finally, the data points are plotted and colored according to the cluster assignment.\n",
    "\n",
    "This is a simple example, but it should give you an idea of how to use the AgglomerativeClustering class. In real-world scenarios, you would probably want to use more data and/or more features, and you might want to experiment with different linkage criteria, such as 'ward', 'complete' or 'average' linkage to see which one works best for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687eccd",
   "metadata": {},
   "source": [
    "# -------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d7f7b",
   "metadata": {},
   "source": [
    "\n",
    "Exploratory Data Analysis: Hierarchical Clustering can be used to explore and understand complex datasets by grouping similar data points together and identifying patterns and relationships within the data.\n",
    "\n",
    "Document Clustering: Hierarchical Clustering can be used to group documents or text based on their content, making it useful for text mining, information retrieval, and more.\n",
    "\n",
    "Image Segmentation: Hierarchical Clustering can be used to segment images by grouping similar pixels together, making it useful for image processing and computer vision tasks.\n",
    "\n",
    "Gene Clustering: Hierarchical Clustering can be used to cluster genes based on their expression levels, making it useful for bioinformatics and genomics research.\n",
    "\n",
    "Astronomy: Hierarchical clustering algorithms have been used in astronomical research to identify galaxy clusters and galaxy groups, which can help to understand the large-scale structure of the universe.\n",
    "\n",
    "Medical imaging: Hierarchical clustering can be used in medical imaging to segment images, such as in magnetic resonance imaging (MRI) and computed tomography (CT) scans, which can be used to help identify and diagnose diseases.\n",
    "\n",
    "As with any unsupervised learning algorithm, it's important to have a good understanding of the domain and the problem at hand in order to interpret the results correctly and make sense of the clusters obtained.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e076c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6895d6",
   "metadata": {},
   "source": [
    "3 )DBSCAN: DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are close to each other in the feature space, while marking as outliers points that are isolated. It's often used for anomaly detection, image segmentation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25da5c",
   "metadata": {},
   "source": [
    "Sure, here's an example of how to use the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm in Python using the DBSCAN class from the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "X = np.random.randn(100, 2)\n",
    "\n",
    "# Create a DBSCAN model and fit it to the data\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Plot the data points and color them according to the cluster assignment\n",
    "import matplotlib.pyplot as plt\n",
    "for i in set(labels):\n",
    "    if i == -1:\n",
    "        color = 'black'\n",
    "    else:\n",
    "        color = 'C' + str(i % 10)\n",
    "    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=color)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d093139",
   "metadata": {},
   "source": [
    "In this example, the DBSCAN class is used to perform density-based clustering on a dataset of 2D points. The eps parameter controls the maximum distance between two samples for them to be considered as in the same neighborhood, and the min_samples parameter controls the number of samples in a neighborhood for a point to be considered as a core point. The model is fit to the data using the fit method. The labels_ attribute of the model is used to get the cluster assignments for each data point. Finally, the data points are plotted and colored according to the cluster assignment, where black dots are noise points.\n",
    "\n",
    "This is a simple example, but it should give you an idea of how to use the DBSCAN class. In real-world scenarios, you would probably want to use more data and/or more features and you might want to experiment with different values of eps and min_samples to see which ones work best for your dataset. Additionally, it's important to note that DBSCAN can identify noise points, which are points that do not belong to any cluster, they are assigned the label -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ebb65",
   "metadata": {},
   "source": [
    "# what are DBSCAN applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4587870",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that can be used for a wide range of applications, some of the most popular include:\n",
    "\n",
    "Anomaly Detection: DBSCAN can be used to identify points that are significantly different from the other points in the dataset, which can be useful for detecting outliers, fraud, or errors.\n",
    "\n",
    "Image Segmentation: DBSCAN can be used to segment images by grouping similar pixels together, making it useful for image processing and computer vision tasks.\n",
    "\n",
    "Astronomy: DBSCAN has been used in astronomical research to identify galaxy clusters and galaxy groups, which can help to understand the large-scale structure of the universe.\n",
    "\n",
    "Medical imaging: DBSCAN can be used in medical imaging to segment images, such as in magnetic resonance imaging (MRI) and computed tomography (CT) scans, which can be used to help identify and diagnose diseases.\n",
    "\n",
    "Geographic data: DBSCAN can be used in geographic data to identify dense areas in a map, such as urban areas, and sparse areas such as rural area, which can be useful for many different fields such as urban planning, retail, and more.\n",
    "\n",
    "Clustering of categorical data: DBSCAN can be used to cluster categorical data, rather than numerical data, by using the similarity measure such as Jaccard similarity or cosine similarity.\n",
    "\n",
    "As with any unsupervised learning algorithm, it's important to have a good understanding of the domain and the problem at hand in order to interpret the results correctly and make sense of the clusters obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93c173",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43d5f3",
   "metadata": {},
   "source": [
    "4) PCA: PCA (Principal Component Analysis) is a dimensionality reduction algorithm that finds the directions of maximum variance in the data and projects the data onto a lower-dimensional space. It's often used for data compression, visualization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5e1a5",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis) is a technique for dimensionality reduction and can be used for a wide range of applications, some of the most popular include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365bc45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Data Visualization: PCA can be used to reduce the dimensionality of high-dimensional datasets and project them onto a lower-dimensional space, making them easier to visualize.\n",
    "\n",
    "Data Compression: PCA can be used to compress high-dimensional data by keeping only the most important features, which can help to reduce the storage and computational requirements of the data.\n",
    "\n",
    "Noise Filtering: PCA can be used to remove noise from the data by identifying and removing the features that are not important, which can improve the performance of other machine learning algorithms.\n",
    "\n",
    "Feature Extraction: PCA can be used to extract the most important features from the data, which can be used as input to other machine learning algorithms, such as supervised learning algorithms.\n",
    "\n",
    "Biology: PCA has been used in bioinformatics to study the gene expression data, which helps to identify the most important genes related to a particular disease.\n",
    "\n",
    "Finance: PCA is used in finance to identify patterns in stock prices and to reduce the number of variables in a portfolio optimization.\n",
    "\n",
    "Speech Recognition: PCA can be used in speech recognition to extract the important features from the audio signal, which can improve the performance of the speech recognition system.\n",
    "\n",
    "Computer Vision: PCA is used in computer vision to extract the important features from images, which can improve the performance of object recognition, face recognition, and more.\n",
    "\n",
    "It's important to keep in mind that PCA is an unsupervised learning technique, which means it does not take the output variable into account, and it is sensitive to scaling. Also, it's important to have a good understanding of the domain and the problem at hand in order to interpret the results correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "X = np.random.randn(100, 5)\n",
    "\n",
    "# Create a PCA model and fit it to the data\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data to the first two principal components\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Plot the transformed data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675e56d",
   "metadata": {},
   "source": [
    "In this example, the PCA class is used to perform PCA on a dataset of 5-dimensional points. The n_components parameter is set to 2, which means that we want to reduce the dimensionality of the data to 2 dimensions. The model is fit to the data using the fit method, and the transform method is used to transform the data to the first two principal components. Finally, the transformed data is plotted using a scatter plot.\n",
    "\n",
    "This is a simple example, but it should give you an idea of how to use the PCA class. In real-world scenarios, you would probably want to use more data and/or more features and you might want to experiment with different number of n_components to see which one works best for your dataset. Additionally, it's important to note that PCA is sensitive to scaling so it's important to scale the data first before applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e81b8",
   "metadata": {},
   "source": [
    "# Sure, here's an example of how to perform PCA (Principal Component Analysis) step by step in Python using numpy and pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f525620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate example data\n",
    "X = np.random.randn(100, 5)\n",
    "\n",
    "# Step 1: Mean centering the data\n",
    "X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "# Step 2: Calculating the covariance matrix\n",
    "cov_matrix = np.cov(X_centered.T)\n",
    "\n",
    "# Step 3: Calculating the eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Step 4: Sorting eigenvectors by decreasing eigenvalues\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Step 5: Selecting the first k eigenvectors\n",
    "k = 2\n",
    "PCA_components = eigenvectors[:, :k]\n",
    "\n",
    "# Step 6: Transforming the data to the first k principal components\n",
    "X_pca = np.dot(X_centered, PCA_components)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80feeb50",
   "metadata": {},
   "source": [
    "In this example, we are performing PCA step by step on a dataset of 5-dimensional points.\n",
    "\n",
    "Mean centering: In this step, we subtract the mean of the data from each data point. This is important because PCA is sensitive to the scale of the data. By mean-centering the data, we ensure that the transformed data has zero mean.\n",
    "\n",
    "Calculating the covariance matrix: We calculate the covariance matrix of the mean-centered data, which is a measure of the variability of each feature with respect to the other features.\n",
    "\n",
    "Calculating the eigenvectors and eigenvalues: We use the numpy function np.linalg.eig to calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors are the principal components and the eigenvalues are the variances of the data along each principal component.\n",
    "\n",
    "Sorting eigenvectors by decreasing eigenvalues: We sort the eigenvectors by decreasing eigenvalues, so that the first principal component has the highest variance, the second principal component has the second highest variance, and so on.\n",
    "\n",
    "Selecting the first k eigenvectors: We select the first k eigenvectors, where k is the number of dimensions we want to reduce the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27ecf2",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a8851",
   "metadata": {},
   "source": [
    "5) t-SNE: t-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction algorithm that is particularly well-suited for visualizing high-dimensional datasets. It's often used for visualizing high-dimensional data, such as images, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "X = np.random.randn(100, 5)\n",
    "\n",
    "# Create a t-SNE model and fit it to the data\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the transformed data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822cfdd2",
   "metadata": {},
   "source": [
    "In this example, the TSNE class is used to perform t-SNE on a dataset of 5-dimensional points. The n_components parameter is set to 2, which means that we want to reduce the dimensionality of the data to 2 dimensions. The model is fit to the data using the fit_transform method and the transformed data is then plotted using a scatter plot.\n",
    "\n",
    "t-SNE is a non-linear dimensionality reduction technique that is particularly well-suited for visualizing high-dimensional data. It is based on the idea of preserving the pairwise distances between the data points in the high-dimensional space, while trying to place them in a low-dimensional space in a way that is best able to preserve these distances.\n",
    "\n",
    "It's important to note that t-SNE is sensitive to the scale of the data, so it's important to scale the data before applying t-SNE. Additionally, t-SNE is sensitive to the choice of parameters, so you may want to experiment with different values of the perplexity and learning_rate parameters to see which ones work best for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd27ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2438c9d5",
   "metadata": {},
   "source": [
    "T-distributed Stochastic Neighbourhood Embedding (tSNE) is an unsupervised Machine Learning algorithm developed in 2008 by Laurens van der Maaten and Geoffery Hinton. It has become widely used in bioinformatics and more generally in data science to visualise the structure of high dimensional data in 2 or 3 dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e46d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d142ddf5",
   "metadata": {},
   "source": [
    "PCA vs t-SNE: t-SNE differs from PCA by preserving only small pairwise distances or local similarities whereas PCA is concerned with preserving large pairwise distances to maximize variance. PCA is a linear dimension reduction technique that seeks to maximize variance and preserves large pairwise distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc66ef",
   "metadata": {},
   "source": [
    "What does a t-SNE plot tell you?\n",
    "\n",
    "With the TSNE plot you can put your mouse over any individual point, which will then cause the name of that point to be drawn under it so you can tell which point is which. You can also tick the labels box to see all sample labels (though this might get a bit messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c9fc2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "t-SNE is also a method to reduce the dimension. One of the most major differences between PCA and t-SNE is it preserves only local similarities whereas PA preserves large pairwise distance maximize variance. It takes a set of points in high dimensional data and converts it into low dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd1ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
