{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd15da8",
   "metadata": {},
   "source": [
    "Storing the data in a CSV file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451401ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# store the data in a CSV file\n",
    "data.to_csv(\"data.csv\", index=False)\n",
    "\n",
    "# load the data from a CSV file\n",
    "loaded_data = pd.read_csv(\"data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697b8b2",
   "metadata": {},
   "source": [
    "Storing the data in a SQLite database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# store the data in a SQLite database\n",
    "with sqlite3.connect(\"data.db\") as con:\n",
    "    data.to_sql(\"data\", con, if_exists=\"replace\")\n",
    "\n",
    "# load the data from a SQLite database\n",
    "loaded_data = pd.read_sql(\"SELECT * FROM data\", sqlite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071345c",
   "metadata": {},
   "source": [
    "Storing the data in a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ded8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# store the data in a pickle file\n",
    "with open(\"data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "# load the data from a pickle file\n",
    "with open(\"data.pkl\", \"rb\") as f:\n",
    "    loaded_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7064ba4",
   "metadata": {},
   "source": [
    "Storing the data in a HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# store the data in a HDF5 file\n",
    "with h5py.File(\"data.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=data)\n",
    "\n",
    "# load the data from a HDF5 file\n",
    "with h5py.File(\"data.h5\", \"r\") as f:\n",
    "    loaded_data = f[\"data\"][:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738dd92",
   "metadata": {},
   "source": [
    "Storing the data in the cloud storage like (aws S3, google cloud storage):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4415eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# store the data in s3\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('data.csv', 'bucket-name', 'data.csv')\n",
    "\n",
    "# load the data from s3\n",
    "s3.download_file('bucket-name', 'data.csv', 'data.csv')\n",
    "loaded_data = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3c5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7808d534",
   "metadata": {},
   "source": [
    "# SPLITTING THE DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1303e",
   "metadata": {},
   "source": [
    "Splitting the data is an important step in the data preparation process, as it allows you to divide your data into two or more sets: a training set, a validation set, and a test set. These sets are used for different purposes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc363f",
   "metadata": {},
   "source": [
    "The training set is used to train the machine learning model.\n",
    "The validation set is used to evaluate the performance of the model and tune the model's hyperparameters.\n",
    "The test set is used to evaluate the performance of the final model and estimate its performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01ce8a",
   "metadata": {},
   "source": [
    "# Here are some examples of how to split the data using the scikit-learn library in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02655272",
   "metadata": {},
   "source": [
    "Splitting the data using the train_test_split() function:\n",
    "    \n",
    "Here, X is the feature data and y is the target data, test_size parameter defines the proportion of the test set, random_state parameter is used to set a seed for the random number generator, so that the split is deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef821f6",
   "metadata": {},
   "source": [
    "Splitting the data using the StratifiedKFold class:\n",
    "\n",
    "    This class splits the data into n_splits folds, where the proportion of samples for each class is roughly the same in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59104d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# create the splitter\n",
    "splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# split the data\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25422c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bd778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57b6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af56a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7517df71",
   "metadata": {},
   "source": [
    "The best storage method for a specific problem and dataset depends on several factors, such as the size and format of the data, the number of users accessing the data, and the budget for storage and bandwidth.\n",
    "\n",
    "Here are some pros and cons of the different storage methods:\n",
    "\n",
    "CSV files:\n",
    "\n",
    "Pros:\n",
    "\n",
    "CSV files are easy to use and understand.\n",
    "CSV files can be easily imported and exported to and from other software, including Excel.\n",
    "CSV files can be easily compressed to save space.\n",
    "\n",
    "Cons:\n",
    "CSV files are not efficient for large datasets and can become slow to read and write.\n",
    "CSV files are not well-suited for concurrent access by multiple users.\n",
    "CSV files are not well-suited for storing binary data.\n",
    "\n",
    "SQLite:\n",
    "\n",
    "Pros:\n",
    "SQLite is a lightweight and easy-to-use relational database management system.\n",
    "SQLite can handle concurrent access by multiple users.\n",
    "SQLite supports advanced querying capabilities.\n",
    "\n",
    "Cons:\n",
    "SQLite is not well-suited for large datasets and can become slow with high write loads.\n",
    "SQLite is not as powerful as more advanced relational databases like MySQL or PostgreSQL.\n",
    "Pickle:\n",
    "\n",
    "Pros:\n",
    "\n",
    "Pickle is useful for storing complex data structures like lists, dicts, and custom classes.\n",
    "Pickle is fast to read and write.\n",
    "Cons:\n",
    "\n",
    "Pickle is not well-suited for concurrent access by multiple users.\n",
    "Pickle files are not human-readable, and it's hard to understand or edit the data stored in them.\n",
    "HDF5:\n",
    "\n",
    "Pros:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HDF5 is well-suited for large datasets and can handle large arrays of numerical data efficiently.\n",
    "\n",
    "HDF5 supports advanced querying and data manipulation capabilities.\n",
    "HDF5 files can be easily compressed to save space.\n",
    "\n",
    "Cons:\n",
    "\n",
    "HDF5 files can be complex to work with and require specialized software to read and write.\n",
    "HDF5 files are not well-suited for concurrent access by multiple users.\n",
    "Cloud storage:\n",
    "\n",
    "Pros:\n",
    "\n",
    "Cloud storage allows you to store your data on remote servers, which can be accessed from anywhere with internet access.\n",
    "Cloud storage providers like AWS S3, google cloud storage offer automatic backups and disaster recovery.\n",
    "Cloud storage can scale to handle large amounts of data and high traffic loads.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Cloud storage can be costly, especially for large amounts of data.\n",
    "Cloud storage can be slower than local storage, depending on internet connection and location.\n",
    "Given all these factors, it's hard to say which storage method is the best for a specific problem and dataset, however, as a general rule of thumb, if you have a small to medium-size dataset and you don't expect high write loads, you can use CSV or SQLite, if you have a large dataset and you expect high write loads, you can use HDF5 or cloud storage, but keep in mind that the cost is a considerable factor when choosing cloud storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623432f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
