{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"market_data.csv\")\n",
    "\n",
    "# Define the features to use for clustering\n",
    "features = [\"Age\", \"Income\", \"Spending\"]\n",
    "\n",
    "# Extract the features from the data\n",
    "X = data[features]\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters = 5\n",
    "\n",
    "# Create a k-means model and fit it to the data\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster assignments for each data point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster assignments to the dataframe\n",
    "data[\"Cluster\"] = labels\n",
    "\n",
    "# Plot the data, color-coded by cluster\n",
    "plt.scatter(data[\"Age\"], data[\"Income\"], c=data[\"Cluster\"])\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86be36",
   "metadata": {},
   "source": [
    "In this example, the k-means algorithm is used to segment a market into 5 clusters based on the age, income and spending of customers. The data is loaded from a CSV file, and the cluster assignments are added to the dataframe. Finally, a scatter plot is created to visualize the data, with the points color-coded by cluster.\n",
    "\n",
    "Once you've segmented your market, you can analyze each cluster to understand the characteristics of the customers in that cluster, and use that information to tailor your marketing efforts to each segment. For example, you might find that one cluster is composed of older, high-income customers who are willing to spend more money, while another cluster is composed of younger, low-income customers who are more price-sensitive.\n",
    "\n",
    "It's also important to note that k-means is not the only algorithm that can be used for market segmentation, other algorithms like Hierarchical clustering, DBSCAN, etc. can be used depending on the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f552e32",
   "metadata": {},
   "source": [
    "Sure, the k-means algorithm can be used for social network analysis by clustering users based on their interactions within the network. For example, you can use the k-means algorithm to identify groups of users who have similar patterns of communication or similar interests.\n",
    "\n",
    "\n",
    "In this example, the k-means algorithm is used to cluster users of a social network into 4 clusters based on the number of messages sent, number of friends, and number of likes. The data is loaded from a CSV file, and the cluster assignments are added to the dataframe. Then, the code loops through each cluster and prints some statistics about it (e.g., the number of users, the average number of messages sent, etc.).\n",
    "\n",
    "It's important to mention that this is a simple example, and in a real-world scenario, you would probably want to use more features and/or more data points. Additionally, the features chosen for clustering are not the only ones that can be used, you can use other features that are relevant to your problem.\n",
    "\n",
    "Once you've segmented your social network into clusters, you can analyze each cluster to understand the characteristics of the users in that cluster and use that information to tailor your social media strategy to each segment. For example, you might find that one cluster is composed of users who are highly active and have a lot of friends, while another cluster is composed of users who are less active and have fewer friends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0305595",
   "metadata": {},
   "source": [
    "# Here's an example of how k-means could be used for search result grouping:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f657ddf",
   "metadata": {},
   "source": [
    "Yes, the k-means algorithm can be used for grouping search results by clustering them based on their similarities. The algorithm can group similar results together, making it easier for users to find what they are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"search_results.csv\")\n",
    "\n",
    "# Define the features to use for clustering\n",
    "features = [\"Title\", \"Description\"]\n",
    "\n",
    "# Create a TfidfVectorizer to extract features from the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Extract the features from the data\n",
    "X = vectorizer.fit_transform(data[\"Title\"] + \" \" + data[\"Description\"])\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters = 5\n",
    "\n",
    "# Create a k-means model and fit it to the data\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster assignments for each search result\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster assignments to the dataframe\n",
    "data[\"Cluster\"] = labels\n",
    "\n",
    "# Group the search results by cluster\n",
    "grouped_results = data.groupby(\"Cluster\")\n",
    "\n",
    "# Print the search results for each cluster\n",
    "for cluster, group in grouped_results:\n",
    "    print(\"Cluster\", cluster)\n",
    "    print(group[[\"Title\", \"Description\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47871d08",
   "metadata": {},
   "source": [
    "In this example, the k-means algorithm is used to group search results into 5 clusters based on the title and description of each result. The data is loaded from a CSV file, and a TfidfVectorizer is used to extract features from the text data. The cluster assignments are added to the dataframe and search results are grouped by cluster. The code then loops through each cluster and prints the search results for that cluster.\n",
    "\n",
    "It's important to mention that this is a simple example and in real-world scenario, you would probably want to use more features and/or more data points, you can also use other feature extraction techniques like BOW, word2vec, etc. depending on your problem.\n",
    "\n",
    "Once you've grouped your search results into clusters, you can present them to the user in a more organized way, for example, by displaying the search results for each cluster in a separate section of the search results page. Additionally, you can analyze each cluster to understand the characteristics of the search results in that cluster and use that information to improve the search algorithm or ranking system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8155c2",
   "metadata": {},
   "source": [
    "# Here's an example of how k-means could be used for medical imaging:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960603a",
   "metadata": {},
   "source": [
    "Yes, the k-means algorithm can be used for medical imaging by clustering images based on their visual features. For example, it can be used to group similar images of a specific disease or condition, or to identify patterns in the images that are indicative of certain conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409328d",
   "metadata": {},
   "source": [
    "In this example, the k-means algorithm is used to group images of faces into 10 clusters based on the visual features of the images. The data is loaded from the \"Labeled Faces in the Wild\" dataset, and PCA is used to reduce the dimensionality of the data. The cluster assignments are added to the dataframe and images are grouped by cluster. The code then loops through each cluster and prints the images for that cluster.\n",
    "\n",
    "It's important to note that this is a simple example and in real-world scenario, you would probably want to use more features and/or more data points. Additionally, you would need to preprocess the medical images to extract the features that you want to cluster on. It's also important to note that this type of clustering is done in image-level, not in pixel-level.\n",
    "\n",
    "Once you've grouped your medical images into clusters, you can analyze each cluster to understand the characteristics of the images in that cluster and use that information to improve the diagnosis process or to develop new methods for image analysis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Load the data\n",
    "data = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce the dimensionality\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters = 10\n",
    "\n",
    "# Create a k-means model and fit it to the data\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Get the cluster assignments for each image\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster assignments to the dataframe\n",
    "data[\"Cluster\"] = labels\n",
    "\n",
    "# Group the images by cluster\n",
    "grouped_images = data.groupby(\"Cluster\")\n",
    "\n",
    "# Print the images for each cluster\n",
    "for cluster, group in grouped_images:\n",
    "    print(\"Cluster\", cluster)\n",
    "    print(group[\"images\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21506c39",
   "metadata": {},
   "source": [
    "# what is reduce the dimensionality?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160e7b37",
   "metadata": {},
   "source": [
    "\n",
    "Reducing the dimensionality of a dataset refers to the process of transforming a dataset with a large number of features into a dataset with a smaller number of features, while still preserving as much of the important information as possible. The goal of dimensionality reduction is to make the dataset more manageable and easier to analyze while still retaining the relevant information for the problem at hand.\n",
    "\n",
    "There are several techniques for dimensionality reduction, some of the most popular include:\n",
    "\n",
    "Principal Component Analysis (PCA): This technique uses linear algebra to transform the dataset into a new set of uncorrelated variables called principal components. The first principal component is the linear combination of the original features that explains the most variance in the data.\n",
    "\n",
    "Linear Discriminant Analysis (LDA): This technique is similar to PCA but it is supervised, it looks for the linear combinations of features that best separate the different classes in the data.\n",
    "\n",
    "Singular Value Decomposition (SVD): This technique is closely related to PCA and also uses linear algebra to transform the data into a new set of uncorrelated variables.\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE): This technique is a non-linear dimensionality reduction method that is particularly well-suited for visualizing high-dimensional datasets.\n",
    "\n",
    "Autoencoder: This technique is a neural network that learns to reconstruct the input data, it encodes the data into a lower-dimensional representation and then decode back to the original dimension, this way it can be used to reduce the dimensionality.\n",
    "\n",
    "These techniques work differently and are appropriate in different scenarios, and choosing the right one depends on the characteristics of the dataset and the problem at hand. Dimensionality reduction can improve the performance of machine learning algorithms, make the data easier to visualize, and help to identify patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# Create a PCA model and fit it to the data\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(data)\n",
    "\n",
    "# Transform the data using the PCA model\n",
    "X_pca = pca.transform(data)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_pca)\n",
    "\n",
    "\n",
    "\n",
    "#In this example, PCA is used to reduce the dimensionality of the data from 2 to 1. \n",
    "#The first principal component is the linear combination of the original features that explains the most variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Generate example data\n",
    "data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Create a LDA model and fit it to the data\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(data, labels)\n",
    "\n",
    "# Transform the data using the LDA model\n",
    "X_lda = lda.transform(data)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_lda)\n",
    "\n",
    "\n",
    "\n",
    "#In this example, LDA is used to reduce the dimensionality of the data from 2 to 1. \n",
    "#It looks for the linear combinations of features that best separate the different classes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Generate example data\n",
    "data = array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a SVD model and fit it to the data\n",
    "svd = TruncatedSVD(n_components=1)\n",
    "svd.fit(data)\n",
    "\n",
    "# Transform the data using the SVD model\n",
    "X_svd = svd.transform(data)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_svd)\n",
    "\n",
    "\n",
    "#In this example, SVD is used to reduce the dimensionality of the data from 3 to 1. It uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear algebra to transform the data into a new set of uncorrelated variables.\n",
    "\n",
    "#t-Distributed Stochastic Neighbor Embedding (t-SNE):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd61156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# Create a t-SNE model and fit it to the data\n",
    "tsne = TSNE(n_components=1)\n",
    "X_tsne = tsne.fit_transform(data)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_tsne)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2474675",
   "metadata": {},
   "source": [
    "In this example, t-SNE is used to reduce the dimensionality of the data from 2 to 1. It's a non-linear dimensionality reduction method that is particularly well-suited for visualizing high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Generate example data\n",
    "data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# Define the number of features in the input and encoded representations\n",
    "input_dim = 2\n",
    "encoding_dim = 1\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "# Define the encoded representation\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Define the decoded representation\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Train the autoencoder on the data\n",
    "autoencoder.fit(data, data, epochs=50, batch_size=32)\n",
    "\n",
    "# Use the encoder part of the autoencoder to reduce the dimensionality of the data\n",
    "encoder = Model(input_layer, encoded)\n",
    "X_encoded = encoder.predict(data)\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cfe6e",
   "metadata": {},
   "source": [
    "In this example, an autoencoder is used to reduce the dimensionality of the data from 2 to 1. Autoencoder is a neural network that learns to reconstruct the input data, it encodes the data into a lower-dimensional representation and then decode back to the original dimension, this way it can be used to reduce the dimensionality. The autoencoder is trained on the data and then the encoder part is used to reduce the dimensionality of the data by encoding it into a lower-dimensional representation.\n",
    "\n",
    "It's important to keep in mind that these are just examples and the dimensionality reduction technique used will depend on the characteristics of the dataset and the problem at hand. Additionally, the number of features to reduce to is a hyper-parameter that should be set based on the trade-off between preserving information and reducing the complexity of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288b6d5",
   "metadata": {},
   "source": [
    "# image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b57e1",
   "metadata": {},
   "source": [
    "# Yes, k-means can be used in image segmentation by clustering the pixels in the image based on their color or texture features. The goal is to group similar pixels together and then use the cluster assignments to segment the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3bd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image = io.imread(\"image.jpg\")\n",
    "\n",
    "# Reshape the image data to be a 2D array of pixels\n",
    "data = image.reshape(-1, 3)\n",
    "\n",
    "# Define the number of clusters\n",
    "n_clusters = 5\n",
    "\n",
    "# Create a k-means model and fit it to the data\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Get the cluster assignments for each pixel\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Reshape the labels to be the same shape as the image\n",
    "segmented_image = labels.reshape(image.shape[0], image.shape[1])\n",
    "\n",
    "# Display the original and segmented images\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.imshow(image)\n",
    "ax1.set_title(\"Original Image\")\n",
    "ax2.imshow(segmented_image)\n",
    "ax2.set_title(\"Segmented Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e4dec",
   "metadata": {},
   "source": [
    "In this example, k-means is used to segment an image into 5 clusters based on the color of each pixel. The image is loaded and reshaped to be a 2D array of pixels. K-means is then used to cluster the pixels based on their RGB values. The cluster assignments are used to segment the image, and the original and segmented images are displayed side by side for comparison.\n",
    "\n",
    "It's important to note that this is a simple example and in real-world scenario, you would probably want to use more features like texture, shape, etc. and/or more data points. Additionally, you would need to preprocess the image to extract the features that you want to cluster on.\n",
    "\n",
    "Once you've segmented the image using k-means, you can use the segmented image to analyze different regions of the image, or to separate objects or regions of interest from the background. Additionally, you can use the segmentation to improve other image processing tasks such as object detection or image enhancement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc8dcd",
   "metadata": {},
   "source": [
    "# anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921d2f5",
   "metadata": {},
   "source": [
    " unsupervised learning can be used for anomaly detection by identifying patterns or deviations in the data that do not conform to the expected behavior. There are several approaches to unsupervised anomaly detection, some of the most popular include:\n",
    "\n",
    "Clustering-Based Anomaly Detection: This approach uses clustering algorithms such as k-means or density-based methods like DBSCAN to group similar data points together. Data points that do not fit well into any of the clusters are considered anomalies.\n",
    "\n",
    "One-Class SVM: This approach builds a model of the normal data and then uses that model to identify data points that deviate from the expected behavior.\n",
    "\n",
    "Autoencoder-Based Anomaly Detection: This approach uses autoencoders to learn a compact representation of the normal data, and then uses the reconstruction error to identify data points that deviate from the expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "X = np.random.normal(size=(200, 2))\n",
    "X = np.r_[X, np.random.normal(size=(20, 2), loc=5)]\n",
    "\n",
    "# Create a one-class SVM model and fit it to the data\n",
    "clf = svm.OneClassSVM(nu=0.05, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X)\n",
    "\n",
    "# Use the model to predict the anomaly scores for the data\n",
    "anomaly_scores = clf.decision_function(X)\n",
    "\n",
    "# Identify the data points with the highest anomaly scores\n",
    "anomalies = X[anomaly_scores < -1e-4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77714a",
   "metadata": {},
   "source": [
    "In this example, a one-class SVM is used to identify anomalies in a dataset of 2D points. The data is generated with 200 points sampled from a normal distribution and 20 points sampled from a different normal distribution with a higher mean. The one-class SVM is trained on the data and then used to predict the anomaly scores for each data point. The data points with the highest anomaly scores are considered anomalies and can be further analyzed.\n",
    "\n",
    "It's important to keep in mind that anomaly detection is a challenging task, and the results often depend on the characteristics of the data and the parameters of the model. Additionally, it's important to have a good understanding of the domain and the problem at hand in order to interpret the results correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f1e2c",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3face",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f250e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689b6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
